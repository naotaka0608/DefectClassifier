"""è©•ä¾¡ãƒšãƒ¼ã‚¸"""

import random
from pathlib import Path
from typing import Any

import numpy as np
import pandas as pd
import plotly.graph_objects as go
import streamlit as st
from sklearn.metrics import (
    accuracy_score,
    confusion_matrix,
    precision_recall_fscore_support,
)

from src.core.category_manager import CategoryManager
from src.core.config import DEFAULT_CATEGORIES_CONFIG
from src.core.constants import ANNOTATIONS_FILE, CHECKPOINTS_DIR, DATA_DIR, BEST_MODEL_PATH, FINAL_MODEL_PATH
from src.core.data_manager import DataManager
from src.core.types import TaskType
from src.inference.predictor import DefectPredictor
from src.training.runner import train_model  # for type hint if needed, or remove


def show_evaluation_page():
    """è©•ä¾¡ãƒšãƒ¼ã‚¸ã‚’è¡¨ç¤º"""
    st.markdown("## ğŸ“Š ãƒ¢ãƒ‡ãƒ«è©•ä¾¡")
    st.markdown("å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡ã§ãã¾ã™ã€‚")

    # ã‚«ãƒ†ã‚´ãƒªãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
    if "category_manager" not in st.session_state:
        st.session_state.category_manager = CategoryManager(DEFAULT_CATEGORIES_CONFIG)

    category_manager = st.session_state.category_manager

    # è©•ä¾¡å®Ÿè¡Œãƒœã‚¿ãƒ³
    if "evaluation_results" not in st.session_state:
        if st.button("è©•ä¾¡ã‚’å®Ÿè¡Œã™ã‚‹", type="primary"):
            with st.spinner("è©•ä¾¡ã‚’å®Ÿè¡Œä¸­... (ã“ã‚Œã«ã¯æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™)"):
                try:
                    results = _run_evaluation(category_manager)
                    st.session_state.evaluation_results = results
                    st.success("è©•ä¾¡ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                    st.rerun()
                except Exception as e:
                    st.error(f"è©•ä¾¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return
    else:
        if st.button("å†è©•ä¾¡ã™ã‚‹"):
            del st.session_state.evaluation_results
            st.rerun()

    if "evaluation_results" in st.session_state:
        results = st.session_state.evaluation_results
        
        # ã‚¿ãƒ–
        tab1, tab2, tab3 = st.tabs(["ğŸ“ˆ ã‚µãƒãƒªãƒ¼", "ğŸ¯ æ··åŒè¡Œåˆ—", "ğŸ“‹ è©³ç´°åˆ†æ"])

        with tab1:
            _show_summary_tab(results, category_manager)

        with tab2:
            _show_confusion_matrix_tab(results, category_manager)

        with tab3:
            _show_detailed_analysis_tab(results, category_manager)


def _run_evaluation(category_manager: CategoryManager) -> dict[str, Any]:
    """è©•ä¾¡ã‚’å®Ÿè¡Œã—ã¦çµæœã‚’è¿”ã™"""
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨åˆ†å‰² (runner.pyã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯)
    data_manager = DataManager(ANNOTATIONS_FILE)
    all_samples = data_manager.load_annotations()
    
    if len(all_samples) == 0:
        raise ValueError("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

    from src.core.data_utils import split_dataset
    _, val_samples = split_dataset(all_samples, train_ratio=0.8, seed=42)

    if not val_samples:
         raise ValueError("æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")

    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    model_path = BEST_MODEL_PATH
    if not model_path.exists():
         model_path = FINAL_MODEL_PATH
         if not model_path.exists():
             raise FileNotFoundError("å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    predictor = DefectPredictor(model_path=model_path, category_manager=category_manager)
    
    # æ¨è«–å®Ÿè¡Œ
    # æ¨è«–å®Ÿè¡Œ
    images = []
    true_labels = {TaskType.CAUSE: [], TaskType.SHAPE: [], TaskType.DEPTH: []}
    
    for sample in val_samples:
        # ç”»åƒãƒ‘ã‚¹è§£æ±º
        if "image_path" in sample:
            img_path = DATA_DIR / sample["image_path"]
        else:
             # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ (dataset.pyã¨åŒæ§˜)
             rel_path = DATA_DIR / "train_images" # ä»®
             # å®Ÿéš›ã®æ§‹æˆã«åˆã‚ã›ã¦èª¿æ•´ãŒå¿…è¦ã ãŒã€dataset.pyã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’è¦‹ã‚‹ã¨
             # TRAIN_IMAGES_DIR.relative_to(DATA_DIR) ã‚’ä½¿ã£ã¦ã„ã‚‹ã€‚
             # ç°¡ç•¥åŒ–ã®ãŸã‚ã€çµ¶å¯¾ãƒ‘ã‚¹ã‚’æ§‹ç¯‰ã—ã¦å­˜åœ¨ç¢ºèª
             # DataManagerãŒä¿å­˜ã—ãŸãƒ‘ã‚¹ã¯ç›¸å¯¾ãƒ‘ã‚¹ã®ã¯ãš
             img_path = Path(DATA_DIR) / sample.get("image_path", "")
             
        if not img_path.exists():
            continue
            
        try:
            from PIL import Image
            img = np.array(Image.open(img_path).convert("RGB"))
            images.append(img)
            true_labels[TaskType.CAUSE].append(sample["cause"])
            true_labels[TaskType.SHAPE].append(sample["shape"])
            true_labels[TaskType.DEPTH].append(sample["depth"])
        except Exception:
            continue

    if not images:
        raise ValueError("æœ‰åŠ¹ãªæ¤œè¨¼ç”¨ç”»åƒãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

    # ãƒãƒƒãƒæ¨è«–
    # ãƒ¡ãƒ¢ãƒªç¯€ç´„ã®ãŸã‚å°åˆ†ã‘ã«ã™ã‚‹
    batch_size = 16
    predictions = []
    for i in range(0, len(images), batch_size):
        batch_images = images[i : i + batch_size]
        batch_preds = predictor.predict_batch(batch_images)
        predictions.extend(batch_preds)

    pred_labels = {
        TaskType.CAUSE: [p.cause.label for p in predictions],
        TaskType.SHAPE: [p.shape.label for p in predictions],
        TaskType.DEPTH: [p.depth.label for p in predictions],
    }
    
    return {
        "true_labels": true_labels,
        "pred_labels": pred_labels,
        "total_samples": len(images)
    }


def _show_summary_tab(results: dict, category_manager: CategoryManager):
    """ã‚µãƒãƒªãƒ¼ã‚¿ãƒ–"""
    st.markdown("### ğŸ“ˆ è©•ä¾¡ã‚µãƒãƒªãƒ¼")
    
    true_labels = results["true_labels"]
    pred_labels = results["pred_labels"]
    
    # å„ã‚¿ã‚¹ã‚¯ã®ç²¾åº¦è¨ˆç®—
    accuracies = {}
    for task in [TaskType.CAUSE, TaskType.SHAPE, TaskType.DEPTH]:
        acc = accuracy_score(true_labels[task], pred_labels[task])
        accuracies[task] = acc

    # å…¨ä½“ç²¾åº¦ï¼ˆå…¨ã‚¿ã‚¹ã‚¯æ­£è§£ï¼‰
    all_correct = 0
    total = results["total_samples"]
    for i in range(total):
        if (true_labels[TaskType.CAUSE][i] == pred_labels[TaskType.CAUSE][i] and
            true_labels[TaskType.SHAPE][i] == pred_labels[TaskType.SHAPE][i] and
            true_labels[TaskType.DEPTH][i] == pred_labels[TaskType.DEPTH][i]):
            all_correct += 1
    overall_acc = all_correct / total if total > 0 else 0

    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚«ãƒ¼ãƒ‰
    cols = st.columns(4)
    metrics = [
        ("å…¨ä½“å®Œå…¨ä¸€è‡´", f"{overall_acc:.1%}", "#667eea"),
        ("åŸå› åˆ†é¡", f"{accuracies[TaskType.CAUSE]:.1%}", "#764ba2"),
        ("å½¢çŠ¶åˆ†é¡", f"{accuracies[TaskType.SHAPE]:.1%}", "#f093fb"),
        ("æ·±ã•åˆ†é¡", f"{accuracies[TaskType.DEPTH]:.1%}", "#5ee7df"),
    ]

    for col, (name, value, color) in zip(cols, metrics):
        with col:
            st.markdown(
                f"""
                <div style="
                    background: linear-gradient(135deg, {color} 0%, {color}99 100%);
                    padding: 1.5rem;
                    border-radius: 1rem;
                    color: white;
                    text-align: center;
                    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
                ">
                    <div style="font-size: 0.9rem; opacity: 0.9;">{name}</div>
                    <div style="font-size: 2rem; font-weight: bold; margin: 0.5rem 0;">{value}</div>
                </div>
                """,
                unsafe_allow_html=True,
            )

    st.markdown("---")

    # ã‚¯ãƒ©ã‚¹åˆ¥ç²¾åº¦ (F1 Score)
    st.markdown("### ğŸ¯ ã‚¯ãƒ©ã‚¹åˆ¥ F1ã‚¹ã‚³ã‚¢")
    task_names = {TaskType.CAUSE: "åŸå› ", TaskType.SHAPE: "å½¢çŠ¶", TaskType.DEPTH: "æ·±ã•"}

    for task, name in task_names.items():
        labels = category_manager.get_categories(task)
        # scikit-learn ã§è¨ˆç®—
        p, r, f1, s = precision_recall_fscore_support(
            true_labels[task], 
            pred_labels[task], 
            labels=labels, 
            zero_division=0
        )
        
        fig = go.Figure(
            data=[
                go.Bar(
                    x=labels,
                    y=[score * 100 for score in f1],
                    marker_color=[
                        f"hsl({i * 360 / len(labels)}, 70%, 60%)"
                        for i in range(len(labels))
                    ],
                    text=[f"{score * 100:.1f}%" for score in f1],
                    textposition="auto",
                )
            ]
        )

        fig.update_layout(
            title=f"{name}åˆ†é¡ (F1 Score)",
            xaxis_title="ã‚«ãƒ†ã‚´ãƒª",
            yaxis_title="F1 Score (%)",
            yaxis_range=[0, 100],
            height=300,
            margin=dict(l=40, r=40, t=60, b=40),
        )
        st.plotly_chart(fig, width="stretch")


def _show_confusion_matrix_tab(results: dict, category_manager: CategoryManager):
    """æ··åŒè¡Œåˆ—ã‚¿ãƒ–"""
    st.markdown("### ğŸ¯ æ··åŒè¡Œåˆ—")

    task_names = {TaskType.CAUSE: "åŸå› ", TaskType.SHAPE: "å½¢çŠ¶", TaskType.DEPTH: "æ·±ã•"}
    selected_task = st.selectbox(
        "åˆ†é¡ã‚¿ã‚¹ã‚¯ã‚’é¸æŠ",
        options=list(task_names.keys()),
        format_func=lambda x: task_names[x],
    )

    categories = category_manager.get_categories(selected_task)
    y_true = results["true_labels"][selected_task]
    y_pred = results["pred_labels"][selected_task]

    cm = confusion_matrix(y_true, y_pred, labels=categories)
    
    # æ­£è¦åŒ– (è¡Œæ–¹å‘ã®å’Œã§å‰²ã‚‹ = Recallçš„ãªè¦–ç‚¹)
    with np.errstate(divide='ignore', invalid='ignore'):
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        cm_normalized = np.nan_to_num(cm_normalized)

    # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
    fig = go.Figure(
        data=go.Heatmap(
            z=cm_normalized,
            x=categories,
            y=categories,
            colorscale="Blues",
            text=cm,
            texttemplate="%{text}",
            textfont={"size": 12},
            hovertemplate="å®Ÿéš›: %{y}<br>äºˆæ¸¬: %{x}<br>ä»¶æ•°: %{text}<br>å‰²åˆ: %{z:.1%}<extra></extra>",
        )
    )

    fig.update_layout(
        title=f"{task_names[selected_task]}åˆ†é¡ã®æ··åŒè¡Œåˆ—",
        xaxis_title="äºˆæ¸¬ãƒ©ãƒ™ãƒ«",
        yaxis_title="å®Ÿéš›ã®ãƒ©ãƒ™ãƒ«",
        height=500,
        margin=dict(l=40, r=40, t=60, b=40),
        yaxis=dict(autorange="reversed") # yè»¸ã‚’ä¸Šã‹ã‚‰ä¸‹ã®é †ã«
    )

    st.plotly_chart(fig, width="stretch")
    
    # çµ±è¨ˆæƒ…å ±
    total = cm.sum()
    correct = np.trace(cm)
    accuracy = correct / total if total > 0 else 0
    
    st.metric(f"{task_names[selected_task]}åˆ†é¡ã®ç²¾åº¦", f"{accuracy:.1%}")


def _show_detailed_analysis_tab(results: dict, category_manager: CategoryManager):
    """è©³ç´°åˆ†æã‚¿ãƒ–"""
    st.markdown("### ğŸ“‹ è©³ç´°åˆ†æ")

    task_names = {TaskType.CAUSE: "åŸå› ", TaskType.SHAPE: "å½¢çŠ¶", TaskType.DEPTH: "æ·±ã•"}
    
    report_data = []

    for task, name in task_names.items():
        st.markdown(f"#### {name}åˆ†é¡")
        categories = category_manager.get_categories(task)
        
        y_true = results["true_labels"][task]
        y_pred = results["pred_labels"][task]
        
        # scikit-learn ã§è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆè¨ˆç®—
        p, r, f1, s = precision_recall_fscore_support(
            y_true, y_pred, labels=categories, zero_division=0
        )
        
        task_data = []
        for i, cat in enumerate(categories):
            row = {
                "ã‚¿ã‚¹ã‚¯": name,
                "ã‚«ãƒ†ã‚´ãƒª": cat,
                "Precision": f"{p[i]:.1%}",
                "Recall": f"{r[i]:.1%}",
                "F1 Score": f"{f1[i]:.1%}",
                "ã‚µãƒ³ãƒ—ãƒ«æ•°": int(s[i]),
            }
            task_data.append(row)
            report_data.append(row)

        df = pd.DataFrame(task_data).drop(columns=["ã‚¿ã‚¹ã‚¯"])
        st.dataframe(df, width="stretch", hide_index=True)
        st.markdown("---")
        
    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿
    if report_data:
        full_df = pd.DataFrame(report_data)
        csv = full_df.to_csv(index=False).encode('utf-8-sig')
        
        st.download_button(
            "ğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=csv,
            file_name="evaluation_report.csv",
            mime="text/csv",
        )
